data:
  provider: yfinance
  default_start_date: "2020-01-01"
  cache_dir: data/raw

news:
  rss_base_url: "https://news.google.com/rss/search"
  queries:
    - "Indian stock market"
    - "NSE BSE stocks"
    - "NIFTY 50"
    - "Indian economy"
  max_articles_per_query: 50
  cache_dir: data/news_cache
  cache_expiry_hours: 6

sentiment:
  model_name: "ProsusAI/finbert"
  batch_size: 32

ner:
  spacy_model: "en_core_web_sm"

llm:
  provider: ollama
  ollama:
    model: "qwen2.5:7b"
    base_url: "http://localhost:11434"
    timeout: 120
  cache_dir: data/news_cache
  cache_expiry_hours: 24

features:
  technical:
    rsi_period: 14
    macd_fast: 12
    macd_slow: 26
    macd_signal: 9
    bb_period: 20
    bb_std: 2
    sma_periods: [20, 50]
    ema_periods: [12, 26]
    atr_period: 14
    stoch_period: 14
  sequence_length: 60
  prediction_horizon: 5  # trading days ahead to predict; must be 1-10
  # Quarterly financial report features
  use_financials: true
  financial_announcement_lag_days: 45  # days after quarter-end before results assumed available
                                        # (used when actual announcement date is unknown)
                                        # Typical for NIFTY 50: 30-60 days; 45 is conservative

models:
  lstm:
    hidden_size: 128
    num_layers: 2
    dropout: 0.3
    learning_rate: 0.001
    epochs: 50
    batch_size: 32
    patience: 10
  xgboost:
    n_estimators: 500
    max_depth: 6
    learning_rate: 0.05
    early_stopping_rounds: 20
    subsample: 0.8
    colsample_bytree: 0.8
  encoder_decoder:
    hidden_size: 128
    num_layers: 2
    dropout: 0.3
    learning_rate: 0.001
    epochs: 50
    batch_size: 32
    patience: 10
  prophet: {}  # no extra hyperparameters needed; tuning done via changepoint_prior_scale grid
  qlearning:
    # Features used to build the discrete state space.
    # Must be present in the feature matrix produced by FeaturePipeline.
    state_features: ["RSI", "MACD_Histogram", "BB_Width", "Volume_Ratio", "Price_Momentum_5d"]
    n_bins: 3              # quantile buckets per state feature (3 bins → 3^5=243 states)
    learning_rate: 0.1     # Q-learning α
    discount_factor: 0.95  # future reward discount γ
    epsilon_start: 1.0     # initial exploration rate
    epsilon_end: 0.01      # minimum exploration rate
    epsilon_decay: 0.995   # per-episode ε decay factor
    n_episodes: 10         # passes over training time-series
    transaction_cost: 0.001  # 0.1% per trade (open or close)
    temperature: 0.5       # softmax temperature: lower = more peaked predictions
  tft:
    hidden_size: 128
    num_heads: 4           # must divide hidden_size evenly; auto-reduced to 1 if not
    num_lstm_layers: 2
    dropout: 0.1
    learning_rate: 0.001
    epochs: 50
    batch_size: 32
    patience: 10
  dqn:
    hidden_sizes: [256, 128]   # MLP hidden layer widths (deepened by adding more entries)
    dropout: 0.2
    learning_rate: 0.001
    batch_size: 64             # replay-buffer mini-batch size
    buffer_size: 10000         # maximum transitions stored in the replay buffer
    min_buffer_size: 64        # min buffer occupancy before gradient updates begin
    target_update_freq: 100    # gradient steps between target-network syncs
    discount_factor: 0.99      # future-reward discount γ (higher than tabular QL)
    epsilon_start: 1.0         # initial exploration rate
    epsilon_end: 0.01          # final exploration rate
                               # ε decays LINEARLY per env step from start→end over
                               # the entire training run (N × n_episodes steps), so
                               # the agent fully transitions from exploration to
                               # exploitation regardless of dataset size.
    n_episodes: 10             # passes over training time-series
    transaction_cost: 0.001    # 0.1% per trade (open or close)
    temperature: 0.5           # softmax temperature: lower = more peaked predictions
  ensemble:
    lstm_weight: 0.4
    xgboost_weight: 0.6
  selected_models: ["lstm"]  # list of models to train/use; multiple = ensemble
                             # available: lstm, xgboost, encoder_decoder, prophet, tft, qlearning, dqn
  save_dir: data/models
  train_split: 0.8
  staleness_warning_days: 30

signals:
  confidence_threshold: 0.6
  strong_threshold: 0.8
  short_confidence_threshold: 0.7
  buy_return_threshold: 0.01
  sell_return_threshold: -0.01
  # Per-horizon BUY / SELL thresholds.
  # Format: horizon_days: [buy_threshold, sell_threshold]
  # Based on sqrt(t) volatility scaling from a ±1% base at horizon 1.
  # Edit these to widen/narrow the HOLD band without retraining.
  horizon_thresholds:
    1:  [ 0.010, -0.010]
    2:  [ 0.014, -0.014]
    3:  [ 0.017, -0.017]
    4:  [ 0.020, -0.020]
    5:  [ 0.022, -0.022]
    6:  [ 0.024, -0.024]
    7:  [ 0.026, -0.026]
    8:  [ 0.028, -0.028]
    9:  [ 0.030, -0.030]
    10: [ 0.032, -0.032]

screener:
  volume_spike_threshold: 1.5
  news_volume_spike_threshold: 2.0
  news_lookback_days: 3

report:
  export_dir: data/processed
  formats: ["console", "csv", "json"]

paper_trading:
  ledger_file: data/trades/ledger.json
  report_dir: data/trades
